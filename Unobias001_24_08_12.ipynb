{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN6UZQI+OR0ToXUEUuaeSsU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greatshotkid/greatshotkid/blob/main/Unobias001_24_08_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **Model 001, using simple bi-directional LSTM**"
      ],
      "metadata": {
        "id": "VL3gDM33yP9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing dataset from google drive"
      ],
      "metadata": {
        "id": "Jt1l1H9LlumS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Hj2cMxNl7Si",
        "outputId": "10abaa6f-a826-4fbd-a01c-30b2e709f974"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the zip file"
      ],
      "metadata": {
        "id": "qC2czDr2mMRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/My Dataset.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/dataset\")\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "MBa-YwwxmSPJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing the required libraries**"
      ],
      "metadata": {
        "id": "h5CLcrSyx_jJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install ftfy\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import PyPDF2\n",
        "import ftfy\n",
        "from io import BytesIO\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "FB40FhDtyLxC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fb788d5-c462-45e4-85be-447e7e756b3c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.2.3)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating some constarints**"
      ],
      "metadata": {
        "id": "y0TFLAYhy5q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#EMBEDDING_LEN = 32  # The length of the word embedding vector\n",
        "#BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "a9DBsxAtzFo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS = [\n",
        "      lambda : tf.keras.metrics.TruePositives(name='tp'),\n",
        "      lambda : tf.keras.metrics.FalsePositives(name='fp'),\n",
        "      lambda : tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "      lambda : tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "\n",
        "      lambda : tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      lambda : tf.keras.metrics.Precision(name='precision'),\n",
        "      lambda : tf.keras.metrics.Recall(name='recall'),\n",
        "]\n",
        "\n",
        "def fresh_metrics():\n",
        "    return [metric() for metric in METRICS]"
      ],
      "metadata": {
        "id": "j0Cts7mizOw7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extracting text from the pdf CVs**"
      ],
      "metadata": {
        "id": "SYiPD60U4_UQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(file_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            for page in reader.pages:\n",
        "              extracted_text = page.extract_text()\n",
        "              if extracted_text:\n",
        "                text += ftfy.fix_text(extracted_text) + \"\\n\"\n",
        "\n",
        "    except PyPDF2.errors.PdfReadError as e:\n",
        "        print(f\"Error reading PDF file {file_path}: {e}\")\n",
        "        return \"\" # Return an empty string if an error occurs\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "_A9UDFE2byjo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text preprocessing**"
      ],
      "metadata": {
        "id": "mxdWNIC3dQeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Removing punctuation\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
        "    # Tokenization\n",
        "    words = word_tokenize(text)\n",
        "    # Removing stop words\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    return words"
      ],
      "metadata": {
        "id": "0YEaXWLedc4c"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing file paths using os module"
      ],
      "metadata": {
        "id": "1NZlCSkw2kRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of file paths\n",
        "pdf_file_paths = []\n",
        "\n",
        "\n",
        "\n",
        "# Example: List all files in a directory\n",
        "directory = '/content/dataset/My Dataset'\n",
        "pdf_file_paths = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.pdf')]\n",
        "\n",
        "#for path in pdf_file_paths:\n",
        "    #print(path)\n",
        "\n"
      ],
      "metadata": {
        "id": "a2iWItoq2zu9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting text into integers using Tokenize and Vectorize"
      ],
      "metadata": {
        "id": "XNm9xJw7f7Gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extract and preprocess all CVs\n",
        "list_of_texts = [preprocess_text(extract_text_from_pdf(file_path)) for file_path in pdf_file_paths] # Call the two functions to extract and preprocess\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000)  # You can specify the number of words\n",
        "tokenizer.fit_on_texts(list_of_texts)  # list_of_texts is a list of preprocessed CVs\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences(list_of_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5Bf_ZXpr0M8",
        "outputId": "9bba7acd-e2b1-428e-8372-75d3cbfddb54"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:PyPDF2._reader:Overwriting cache for 0 76\n",
            "WARNING:PyPDF2._reader:Overwriting cache for 0 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading PDF file /content/dataset/My Dataset/3013.pdf: EOF marker not found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:PyPDF2._reader:Overwriting cache for 0 216\n",
            "WARNING:PyPDF2._reader:Overwriting cache for 0 51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading PDF file /content/dataset/My Dataset/1648.pdf: EOF marker not found\n",
            "Error reading PDF file /content/dataset/My Dataset/3012.pdf: EOF marker not found\n",
            "Error reading PDF file /content/dataset/My Dataset/1124.pdf: EOF marker not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding padding"
      ],
      "metadata": {
        "id": "lb3_awH9CDrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the maximum sequence length\n",
        "max_sequence_length = 1000\n",
        "\n",
        "# Pad sequences\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)"
      ],
      "metadata": {
        "id": "icVKbqWvCHCu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating training, testing and validation sets"
      ],
      "metadata": {
        "id": "RA7xlpEYE7UH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Split the data into training (80%) and testing (20%) sets\n",
        " X_train, X_test = train_test_split(padded_sequences, test_size=0.2, random_state=42)\n",
        "\n",
        " # Split the training set into training (75%) and validation (25%) sets\n",
        " X_train, X_val = train_test_split(X_train, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "aVnJ7sH9Fjdz"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}